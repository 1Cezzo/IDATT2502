{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6055b2-8298-4107-8877-575e1b9fdc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Feature  Importance\n",
      "27                      odor_n    0.145422\n",
      "36                 gill-size_n    0.072117\n",
      "24                      odor_f    0.068575\n",
      "35                 gill-size_b    0.055724\n",
      "96         spore-print-color_h    0.050855\n",
      "61  stalk-surface-below-ring_k    0.049323\n",
      "37                gill-color_b    0.038629\n",
      "57  stalk-surface-above-ring_k    0.033862\n",
      "21                   bruises_t    0.029547\n",
      "94                 ring-type_p    0.027991\n",
      "Number of selected features: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to explain 95% variance: 59\n",
      "Top features contributing to variance:\n",
      "gill-color_u     5.196515\n",
      "cap-shape_s      5.126763\n",
      "gill-color_k     4.941647\n",
      "cap-color_b      4.940531\n",
      "cap-surface_f    4.706292\n",
      "cap-color_c      4.638111\n",
      "habitat_m        4.503917\n",
      "cap-surface_y    4.495514\n",
      "cap-color_p      4.468420\n",
      "cap-color_w      4.427859\n",
      "dtype: float64\n",
      "There is not much overlap between features which are most discriminative and those which cause much variance,\n",
      "which could mean that the features that cause much of the variance also do a poor job at being classifiers, \n",
      "so we could remove these and likely not lose much of the accuracy of our model.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "column_names = ['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment',\n",
    "                'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root',\n",
    "                'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
    "                'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type',\n",
    "                'spore-print-color', 'population', 'habitat']\n",
    "\n",
    "data = pd.read_csv('agaricus-lepiota.data', header=None, names=column_names)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "data['class'] = label_encoder.fit_transform(data['class'])\n",
    "\n",
    "data_encoded = pd.get_dummies(data.drop('class', axis=1))\n",
    "\n",
    "X = data_encoded\n",
    "y = data['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "feature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importances.head(10))\n",
    "\n",
    "selector = SelectFromModel(clf, threshold='mean', prefit=True)\n",
    "X_selected = selector.transform(X)\n",
    "\n",
    "print(f\"Number of selected features: {X_selected.shape[1]}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_encoded)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(data_scaled)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "n_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"Number of components to explain 95% variance: {n_components}\")\n",
    "\n",
    "pca_reduced = PCA(n_components=n_components)\n",
    "pca_reduced.fit(data_scaled)\n",
    "\n",
    "loadings = pca_reduced.components_\n",
    "\n",
    "loading_matrix = pd.DataFrame(loadings, columns=data_encoded.columns)\n",
    "\n",
    "feature_importance = loading_matrix.abs().sum(axis=0)\n",
    "\n",
    "top_features = feature_importance.sort_values(ascending=False)\n",
    "\n",
    "print(\"Top features contributing to variance:\")\n",
    "print(top_features.head(10))\n",
    "\n",
    "print(\"There is not much overlap between features which are most discriminative and those which cause much variance,\")\n",
    "print(\"which could mean that the features that cause much of the variance also do a poor job at being classifiers, \")\n",
    "print(\"so we could remove these and likely not lose much of the accuracy of our model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366e876-e186-49df-a144-c7013328b592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
